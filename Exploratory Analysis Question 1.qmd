---
title: "Project 1 Obesity Prevalence"
author: "Shihong"
execute: 
  echo: false
  eval: true
number-sections: true
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| warning: false
#| message: false
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(gt)
library(MASS)
library(patchwork)
library(moderndive)
library(sjPlot)

```

# Introduction {#sec-intro}

Obesity is a significant public health concern that affects morbidity and mortality. Monitoring its prevalence is essential for evaluating public health policies and interventions. The Scottish Health Survey, which samples individuals from private households across Scotland, provides an opportunity to study trends in obesity. The primary goal of this analysis is to determine whether there has been a statistically noticeable change in obesity prevalence in Scotland over the period 2008–2012.

# Exploratory Analysis {#sec-EA}

## Prevalence

The prevalence of obesity for each year was computed using the formula:

$$
\text{Prevalence (in \%)} = \frac{N_{\text{obese}}}{N_{\text{total}}} \times 100
$$

First we will produce a line plot with data points to illustrate the trend in obesity prevalence over the years.

```{r}
data <- read.csv("DAProject1.csv")

for (i in c(2:7)) {
  data[,i] <- factor(data[,i])
}


prevalence <- data %>%
  group_by(Year) %>%
  summarise(total = n(),
            obese_count = sum(Obese == "Yes"),
            prevalence = (obese_count / total) * 100)

ggplot(prevalence, aes(x = Year, y = prevalence)) +
  geom_line(group = 1, color = "black") +
  geom_point(size = 3, color = "red") +
  labs(title = "Obesity Prevalence Over Time in Scotland",
       x = "Year",
       y = "Prevalence (in %)")
```

From the graph we can see that the prevalence appears to fluctuate year to year, rather than moving in a strictly upward or downward path. This suggests that obesity rates may have been influenced by a range of factors that vary from year to year (e.g., sample composition, lifestyle changes, policy impacts).

Also we will produce a bar plot to display the count of individuals classified as obese versus not obese for each survey year.

```{r}
ggplot(data, aes(x = Year, fill = Obese)) +
  geom_bar(position = "dodge") +
  labs(title = "Count of Obesity Classification by Year",
       x = "Year",
       y = "Count") 
```

We can observe that in each year, the “No” bar (non-obese) is taller than the “Yes” bar (obese). However, the gap between these two bars may differ slightly from year to year.

## Differences in Obesity

We can start with a density plot of our continuous numerical 

```{r}

```



# Formal Analysis {#sec-FA}

## Prevalence

The logistic regression model is given by:


\begin{align}
y_i &\sim  \mathrm{Bernoulli}(p_i) \\
\end{align}
\begin{align}
logit(p_i) = \alpha + \beta_{year2009} \cdot \mathbb{I}_{\mathrm{year2009}} + \beta_{year2010} \cdot \mathbb{I}_{\mathrm{year2010}} + \beta_{year2011} \cdot \mathbb{I}_{\mathrm{year2011}} + \beta_{year2012} \cdot \mathbb{I}_{\mathrm{year2012}}
\end{align}

```{r}
#| message: false
#| warning: false

data$Obese <- as.factor(data$Obese)
model <- glm(Obese ~ Year, data = data, family = binomial)
#model %>% tidy(conf.int = TRUE, conf.level = 0.95)
model %>%
  tidy(conf.int = T,
       exponentiate = T) %>% 
  gt()

plot_model(model, show.values = TRUE,
           title = "Odds (being obese compared to Year 2008)", show.p = FALSE)
```

The fitted model is: $$
logit(p_i) = -0.867901  -0.037769 \cdot \mathbb{I}_{\mathrm{year2009}} + 0.043062 \cdot \mathbb{I}_{\mathrm{year2010}} + 0.003814 \cdot \mathbb{I}_{\mathrm{year2011}} + 0.001743 \cdot \mathbb{I}_{\mathrm{year2012}}$$ We have fitted a logistic regression model to observe the relation between the years and obesity prevalance in Scotland over the years 2008-2012.\
The baseline category is taken to be Year 2008. From the output odds model summary, we can see that the odds of being obese in Year 2008 was 0.42.\
The coefficients for Year2009, Year2010, Year2011, Year2012 represents the change in odds of being obese compared to the baseline year 2008.


## Differences

We can attempt to fit a logistic regression model to the data set. With the `stepAIC` function from the `MASS` library, we can perform stepwise model selection to help decide which predictors to include in our GLM.

```{r}
#| echo: false
#| eval: false
#| message: false

model_Full <- glm(Obese~., family = "binomial", data = data)
model_Null <- glm(Obese~1, family = "binomial", data = data)

model_1 <- MASS::stepAIC(model_Null,
                         scope = formula(model_Full),
                         direction = "both")
print(model_1$formula)
```

From this, we see that one potential model fit would be to include the variables Age, Education, and Veg in our GLM. We can also use the `regsubsets` function from the `leaps` package, specifying that evaluations are to be carried out with the Mallow's CP criterion

```{r}
model_1 <- glm(Obese ~ Age + Education + Veg,
               family = binomial,
               data = data)

models <- leaps::regsubsets(Obese ~ . , data = data)
plot(models, scale = "Cp")
```

As these two processes are in agreement, we fit the following model to investigate potential differences in obesity levels by age, socio-economic status or lifestyle factors.

$$ y_i \sim \text{Bernoulli}(p_i)$$ 
$$\text{log}(\frac{p_i}{1-p_i}) = 
\alpha + \beta_\text{age} \cdot x_i + 
\beta_\text{veg} \cdot \mathbb{I}_\text{veg} +
\beta_\text{none} \cdot \mathbb{I}_\text{none} + 
\beta_\text{standard} \cdot \mathbb{I}_\text{standard} + 
\beta_\text{higher} \cdot \mathbb{I}_\text{higher} + 
\beta_\text{HNC/D} \cdot \mathbb{I}_\text{HNC/D} +
\beta_\text{other} \cdot \mathbb{I}_\text{other}$$
where 

- $p_i$ is the probability of observation $\text{i}$ being classified as obese
- $\alpha$ is the intercept term of our model
- $\beta_\text{age}$ is the coefficient of our continuous variable, age
- $x_i$ is the age of observation $\text{i}$
- $\beta_j$ is the additional intercept of an observation that falls into category $\text{j}$
- $\mathbb{I}_j$ is the indicator variable for category j such that 
$$
\mathbb{I}_j = \cases{1 & \text{for observations that fall into category j} \\
0 & \text{otherwise}}
$$
With our model now fitted, we can obtain our model summary table of the odds estimates of each predictor.

```{r}
#| tbl-cap: Model output of odds, with 95% confidence interval
#| label: tbl-diff-mod-output

model_1 %>%
  tidy(conf.int = TRUE, exponentiate = TRUE) %>%
  dplyr::select(-p.value) %>%
  gt() %>%
  fmt_number(decimals = 3) %>%
  text_case_match(
    "(Intercept)" ~ "Intercept",
    "EducationHigher grade or equiv" ~ "Higher or equivalent",
    "EducationHNC/D or equiv" ~ "HNC/D or equivalent",
    "EducationNo qualifications" ~ "No qualifications",
    "EducationOther school level" ~ "Other school level",
    "EducationStandard grade or equiv" ~ "Standard grade or equivalent"	
  )
```

From this, we see that our model predicts that a one year increase (all other variables held constant) in age brings 1.012 times the odds of obesity. Similarly, we see that a person who consumes the recommended amount of vegetables per day has 0.87 times the odds of obesity, compared to a person who does not. Finally, we see that those with degrees are predicted to have lower odds of obesity when compared to every other education class.

We note that none of the 95% confidence intervals of our chosen predictors contain 1, which is a positive indicator of the predictors' significance.

We can examine our model's predictive performance by first examining the below plots of predicted obesity probabilities against each explanatory variable, as seen in @fig-diff-pred-plot.

```{r}
#| label: fig-diff-pred-plot
#| fig-cap: Predictive plots of obesity against each explanatory variable
#| fig-cap-location: top

pred_plot_age <- plot_model(model_1, 
           type = "pred", 
           title = "", 
           terms="Age [all]", 
           axis.title = c("Age", 
                          "Obesity probability"))

pred_plot_edu <- plot_model(model_1, 
           type = "pred", 
           title = "", 
           terms="Education [all]", 
           axis.title = c("Education", 
                          "Obesity probability")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

pred_plot_veg <- plot_model(model_1, 
           type = "pred", 
           title = "", 
           terms="Veg [all]", 
           axis.title = c("Vegetable intake?", 
                          "Obesity probability"))

gridExtra::grid.arrange(pred_plot_age,
                        pred_plot_edu,
                        pred_plot_veg,
                        ncol = 3, nrow = 1)

#nrow(data[data$Obese == "Yes",]) / nrow(data)

```

We first note that our findings from the model output table relating to trends in odds / probabilities for each variable are confirmed here. However, we also can note that our model seems hesitant to assign a probability greater than 0.5 to any observation in our data set. To expand on this, we first can check our model assumptions via the `performance` library.

```{r}
#| label: fig-diff-asump-plot
#| fig-cap: Various diagnostic plots for checking our model assumptions
#| eval: false
#| echo: true

performance::check_model(model_1, panel = T)
```

From this output, we see that all of our assumptions appear to hold, except for the binned residuals plot, where ~69% of our binned residuals fall outwith the error bounds. This could be due to a number of reasons, such as under-fitting, potential interaction terms not yet considered, or an absence of important explanatory variables not included in our data set. It is important to note that ~70% of our observations are in the non-obese category, which could explain this result.

Continuing, we can also further investigate the performance of our model by plotting the model's ROC curve.

```{r}
#| label: fig-diff-roc-plot
#| fig-cap: ROC curve of our classification model
#| fig-cap-location: top

model_1_pred <- model_1 %>%
  augment(type.predict = c("response"))

model_pred_0.5 <- model_1_pred %>%
  mutate(predicted_class = 
           factor(ifelse(.fitted > 0.5, "Yes", "No")))

roc_model <- roc_curve(model_pred_0.5,
                         truth = Obese,
                         .fitted,
                         event_level = "second")  %>% 
  mutate(youden_j = sensitivity + specificity - 1)

plot(x = 1:0, y = 0:1, type = "n", 
     xlab = "1 - Specificity", ylab = "Sensitivity")
lines(x = 1 - roc_model$specificity, y = roc_model$sensitivity,
      type = "l", lwd = 2)
abline(a = 0, b = 1)
#abline(v = c(0.464, 0.269))
```

We see from @fig-diff-roc-plot that as a whole our model seems to be slightly better than just randomly guessing, but we can further examine this by looking at the observations predicted to be most likely to be obese.

```{r}
#| label: fig-diff-pred-model
#| fig-cap: Predicted values of observations
#| fig-cap-location: top

pred_tab_head <- model_1_pred[order(model_1_pred$.fitted, decreasing = T),] %>%
  dplyr::select(Obese, Age, Education, Veg, .fitted) %>%
  slice_head(n = 5)
pred_tab_head$.fitted <- round(pred_tab_head$.fitted, 3)

pred_tab_head %>%
  gt() %>%
  cols_label(
    .fitted = html("Pred. Prob.")
  )
```

@fig-diff-pred-model shows that our model never predicts an observation to have a probability of obesity greater than 48.1%. So to examine the predictive power of our model, we must set a threshold below 0.5. According to the Youden index summary metric, the threshold that maximises the specificity and sensitivity is 0.269. At this threshold, we obtain the following confusion matrix and performance metrics, seen in @fig-diff-conf-mat.

```{r}
#| label: fig-diff-conf-mat
#| fig-cap: Confusion matrix at a threshold of 0.269
#| fig-cap-location: top

model_pred_0.269 <- model_1_pred %>%
  mutate(predicted_class = 
           factor(ifelse(.fitted > 0.269, "Yes", "No")))

eval_metric <- metric_set(accuracy,sensitivity,specificity,ppv,npv)

conf_mat(model_pred_0.269, 
         truth = Obese, 
         estimate = predicted_class)[[1]] %>%
  as.data.frame() %>%
  gt() %>%
  tab_style(
    style = cell_text(align = "center"),
    location = cells_body()
  ) #%>%
#  data_color(
#    rows = c(1, 4),
#    palette = "green4",
#    alpha = 0.4
#    ) %>%
#  data_color(
#    rows = c(2,3),
#    palette = "red2",
#    alpha = 0.4
#  )
```

```{r}
#| label: fig-diff-perf-metrics
#| fig-cap: Predictive performance metrics for our model at a threshold of 0.269
#| fig-cap-location: top

eval_metric(model_pred_0.269,
            truth = Obese,
            estimate = predicted_class,  
            event_level = "second") %>% 
  dplyr::select(.metric, .estimate) %>%
  gt() %>%
  cols_label(
    .metric = html("Metric"),
    .estimate = html("Estimate")
  )
```

These metrics reveal that our model's true negative rate of prediction is an acceptable level, it struggles to correctly predict when an observation is obese, so our model struggles to differentiate obesity observations based on age, socio-economic status or lifestyle factors.

# Conclusions {#sec-con}

## Differences

In conclusion, our model struggled to effectively differentiate between obese and non-obese observations based on the provided predictors. Our exploratory analysis suggested no notable difference in obesity levels between genders, which was confirmed when fitting our model. While our model appears to suggest that age, education level and vegetable consumption are all statistically significant, it is possible that another model with extra explanatory variables could have greater predictive power, which could be worth future analysis. Additionally, it could be worth considering whether our obtained results would have differed had we split our data into a training and testing split initially. 